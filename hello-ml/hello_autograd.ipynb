{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\monte\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "######################################################################\n",
    "# 1. Define the function and give initial values to the parameters\n",
    "######################################################################\n",
    "\n",
    "# Define the function to minimize: f(x) = 3x^2 + 2x \n",
    "# (Derivative is 6x + 2, minium is at x = -1/3)\n",
    "def f(x):\n",
    "    return 3*x**2 + 2*x \n",
    "\n",
    "# Give an initial value for the variable x\n",
    "x = torch.tensor([7.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last function in graph: <AddBackward0 object at 0x00000270F584AD30>\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# 2. Inspect the computation graph\n",
    "######################################################################\n",
    "\n",
    "# When we evaluate f with an input marked \"requires_grad=True\", Pytorch builds a\n",
    "# graph with all of the component functions of f. For f(x) = 3x^2 + 2x, the\n",
    "# graph should have 4 nodes: 1 for the exponent, 2 for the multiplications,\n",
    "# and 1 for the addition. We can inspect the graph using the grad_fn attribute:\n",
    "print(\"Last function in graph:\", f(x).grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preceding functions: ((<MulBackward0 object at 0x00000270F57E2790>, 0), (<MulBackward0 object at 0x00000270F57E2A60>, 0))\n"
     ]
    }
   ],
   "source": [
    "print(\"Preceding functions:\", f(x).grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the full graph\n",
    "graph = make_dot(f(x), params={'x': x})\n",
    "# graph.render('graph', display=True)\n",
    "graph.render('graph', format='png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the derivative 6x + 2? tensor([True])\n"
     ]
    }
   ],
   "source": [
    "# backward() computes the chain rule for all the component functions of f,\n",
    "# evaluates this at the current value of x, and stores the result in x.grad\n",
    "f(x).backward()\n",
    "\n",
    "# Check if pytorch computed the derivative we expected\n",
    "print(\"Is the derivative 6x + 2?\", 6*x + 2 == x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial values: f(x) = 161.0000, x = 7.0000\n",
      "Iteration 1: f(x) = 6.1200, x = -1.8000\n",
      "Iteration 2: f(x) = 0.6992, x = -0.9200\n",
      "Iteration 3: f(x) = -0.1681, x = -0.5680\n",
      "Iteration 4: f(x) = -0.3069, x = -0.4272\n",
      "Iteration 5: f(x) = -0.3291, x = -0.3709\n",
      "Iteration 6: f(x) = -0.3327, x = -0.3484\n",
      "Iteration 7: f(x) = -0.3332, x = -0.3393\n",
      "Iteration 8: f(x) = -0.3333, x = -0.3357\n",
      "Iteration 9: f(x) = -0.3333, x = -0.3343\n",
      "Iteration 10: f(x) = -0.3333, x = -0.3337\n",
      "Iteration 11: f(x) = -0.3333, x = -0.3335\n",
      "Iteration 12: f(x) = -0.3333, x = -0.3334\n",
      "Iteration 13: f(x) = -0.3333, x = -0.3334\n",
      "Iteration 14: f(x) = -0.3333, x = -0.3333\n",
      "Iteration 15: f(x) = -0.3333, x = -0.3333\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# 3. Perform gradient descent\n",
    "#####################################################################\n",
    "\n",
    "# Choose Stochastic Gradient Descent as the optimization algorithm\n",
    "optimizer = torch.optim.SGD(params=[x], lr=0.1)\n",
    "\n",
    "# Print initial values\n",
    "print(f\"Initial values: f(x) = {f(x).item():.4f}, x = {x.item():.4f}\")\n",
    "\n",
    "for i in range(15):\n",
    "    # Use backward() to compute the gradient of f at the current value of x\n",
    "    f(x).backward()\n",
    "    \n",
    "    # Update the variable x using the SGD optimizer\n",
    "    # (Subtract the gradient from the current value of x)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Zero the gradient for the next iteration\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Print current values\n",
    "    print(f\"Iteration {i+1}: f(x) = {f(x).item():.4f}, x = {x.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
